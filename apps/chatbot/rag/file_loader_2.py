from langchain_core.documents import Document
from langchain_pymupdf4llm import PyMuPDF4LLMLoader
from chonkie import OpenAIEmbeddings
from chonkie import SDPMChunker

from standardize import preprocess_text

import glob
import os
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Union
import time
from pathlib import Path
import json
import re

load_dotenv()

api_key = os.getenv("OPENAIAPI_KEY")

DEFAULT_CHUNK_SIZE = 800
DEFAULT_CHUNK_OVERLAP = 200

class BaseLoader:
    def __init__(self) -> None:
        pass

    def __call__(self, file_path: str, **kwargs):
        pass

class PDFLoader(BaseLoader):
    def __init__(self) -> None:
        super().__init__()

    def __call__(self, file_path: str, **kwargs):
        print(f"ƒêang x·ª≠ l√Ω file PDF: {file_path}")
        doc_loaded = self._load_pdf(file_path)
        print(f"ƒê√£ t·∫£i xong file PDF: {os.path.basename(file_path)}, s·ªë trang: {len(doc_loaded)}")
        return doc_loaded
    
    def _load_pdf(self, file_path):
        try:
            print(f"ƒêang c·ªë g·∫Øng t·∫£i file PDF t·ª´: {os.path.basename(file_path)}")
            pages = PyMuPDF4LLMLoader(file_path).load()
            print(f"T·∫£i th√†nh c√¥ng {os.path.basename(file_path)} v·ªõi {len(pages)} trang")
        except Exception as e:
            print(f"L·ªói khi t·∫£i file PDF {os.path.basename(file_path)}: {str(e)}")
            return []

        print(f"ƒêang chuy·ªÉn ƒë·ªïi trang th√†nh Document cho {os.path.basename(file_path)}")
        documents = [
            Document(
                page_content=preprocess_text(page.page_content),
                metadata={"page": idx + 1, "source": file_path, "filename": os.path.basename(file_path)}
            )
            for idx, page in enumerate(pages)
        ]
        print(f"ƒê√£ t·∫°o {len(documents)} Documents cho {os.path.basename(file_path)}")
        return documents

class TextSplitter:
    def __init__(self, embedding=OpenAIEmbeddings(api_key="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",
                                            model="text-embedding-3-large")):
        self.chunker = SDPMChunker(embedding_model=embedding,  
                                   threshold=0.6,
                                   chunk_size=512,
                                   min_sentences=2,
                                   skip_window=2)
    def __call__(self, documents):  
        all_chunks = []
        for doc in documents:
            chunks = self.chunker.chunk(doc.page_content)
            # Chuy·ªÉn ƒë·ªïi chunks th√†nh Document objects v·ªõi metadata
            for chunk in chunks:
                all_chunks.append(Document(
                    page_content=chunk.text,
                    metadata=doc.metadata
                ))
        return all_chunks

class MarkdownLoader(BaseLoader):
    def __init__(self) -> None:
        super().__init__()

    def __call__(self, file_path: str, **kwargs):
        print(f"ƒêang x·ª≠ l√Ω file Markdown: {file_path}")
        doc_loaded = self._load_markdown(file_path)
        print(f"ƒê√£ t·∫£i xong file Markdown: {os.path.basename(file_path)}, s·ªë chunks: {len(doc_loaded)}")
        return doc_loaded
    
    def _load_markdown(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return self._parse_dgl_table(content, file_path)
        except Exception as e:
            print(f"L·ªói khi t·∫£i file Markdown {os.path.basename(file_path)}: {str(e)}")
            return []
    
    def _parse_dgl_table(self, content, file_path):
        """Parse b·∫£ng DGL th√†nh chunks c√≥ c·∫•u tr√∫c"""
        lines = content.strip().split('\n')
        documents = []
        
        # T√¨m header v√† separator
        header_line = None
        separator_line = None
        data_start = 0
        
        for i, line in enumerate(lines):
            if '| UN No.' in line:
                header_line = line
                if i + 1 < len(lines) and '|------' in lines[i + 1]:
                    separator_line = lines[i + 1]
                    data_start = i + 2
                break
        
        if not header_line:
            print("Kh√¥ng t√¨m th·∫•y header c·ªßa b·∫£ng DGL")
            return documents
        
        # Parse header columns
        headers = [col.strip() for col in header_line.split('|') if col.strip()]
        
        # Parse data rows
        for i in range(data_start, len(lines)):
            line = lines[i].strip()
            if not line or not line.startswith('|'):
                continue
                
            # Parse row data
            row_data = [col.strip() for col in line.split('|') if col.strip() != '']
            
            if len(row_data) < 2:  # Skip invalid rows
                continue
            
            # T·∫°o chunk cho m·ªói h√†ng
            chunk_content = self._create_structured_chunk(headers, row_data, file_path)
            
            if chunk_content:
                documents.append(Document(
                    page_content=chunk_content,
                    metadata={
                        "source": file_path,
                        "filename": os.path.basename(file_path),
                        "un_number": row_data[0] if row_data else "Unknown",
                        "substance_name": row_data[1] if len(row_data) > 1 else "Unknown",
                        "type": "dangerous_goods_list"
                    }
                ))
        
        return documents
    
    def _create_structured_chunk(self, headers, row_data, file_path):
        """T·∫°o chunk c√≥ c·∫•u tr√∫c t·ª´ m·ªôt h√†ng d·ªØ li·ªáu"""
        try:
            chunk_lines = [
                "DANGEROUS GOODS LIST - IMDG CODE",
                "=" * 40
            ]
            
            # Map data to headers
            for i, header in enumerate(headers[:len(row_data)]):
                if i < len(row_data) and row_data[i]:
                    chunk_lines.append(f"{header}: {row_data[i]}")
            
            chunk_lines.append("")
            chunk_lines.append(f"Source: {os.path.basename(file_path)}")
            
            return "\n".join(chunk_lines)
            
        except Exception as e:
            print(f"L·ªói khi t·∫°o chunk: {str(e)}")
            return None

class DGLTextLoader(BaseLoader):
    def __init__(self) -> None:
        """
        Loader chuy√™n bi·ªát cho file text ch·ª©a Dangerous Goods List
        v·ªõi ƒë·ªãnh d·∫°ng OBJECT + original_data + summarization
        """
        super().__init__()

    def __call__(self, file_path: str, **kwargs):
        print(f"ƒêang x·ª≠ l√Ω file DGL Text: {file_path}")
        doc_loaded = self._load_dgl_text(file_path)
        print(f"ƒê√£ t·∫£i xong file DGL Text: {os.path.basename(file_path)}, s·ªë chunks: {len(doc_loaded)}")
        return doc_loaded
    
    def _load_dgl_text(self, file_path):
        """Load file text v√† chia th√†nh chunks theo object"""
        try:
            print(f"ƒêang ƒë·ªçc file: {os.path.basename(file_path)}")
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return self._parse_dgl_objects(content, file_path)
            
        except Exception as e:
            print(f"L·ªói khi t·∫£i file DGL Text {os.path.basename(file_path)}: {str(e)}")
            return []
    
    def _parse_dgl_objects(self, content, file_path):
        """Parse c√°c object DGL t·ª´ text"""
        documents = []
        
        # T√°ch theo pattern "OBJECT"
        object_pattern = r'OBJECT\s+(\d+/\d+)\s*\n'
        objects = re.split(object_pattern, content)
        
        # B·ªè qua ph·∫ßn ƒë·∫ßu (tr∆∞·ªõc object ƒë·∫ßu ti√™n)
        if objects and not objects[0].strip().startswith('OBJECT'):
            objects = objects[1:]
        
        # X·ª≠ l√Ω t·ª´ng c·∫∑p (object_id, object_content)
        for i in range(0, len(objects), 2):
            if i + 1 < len(objects):
                object_id = objects[i].strip()
                object_content = objects[i + 1].strip()
                
                # Parse object content
                parsed_data = self._parse_single_object(object_id, object_content)
                
                if parsed_data:
                    # T·∫°o chunk ch·ªâ v·ªõi summarization (ƒë∆°n gi·∫£n)
                    chunk_content = self._create_simple_chunk(parsed_data, file_path)
                    
                    if chunk_content:
                        doc = Document(
                            page_content=chunk_content,
                            metadata={
                                "un_number": parsed_data.get("un_number"),
                                "substance_name": parsed_data.get("substance_name"),
                            }
                        )
                        documents.append(doc)
        
        print(f"ƒê√£ parse {len(documents)} objects t·ª´ file {os.path.basename(file_path)}")
        return documents
    
    def _parse_single_object(self, object_id, content):
        """Parse m·ªôt object ƒë∆°n l·∫ª"""
        try:
            lines = content.split('\n')
            
            # T√¨m d√≤ng original_data v√† summarization
            original_data = None
            summarization = None
            
            for line in lines:
                if line.startswith('original_data:'):
                    json_str = line.replace('original_data:', '').strip()
                    try:
                        original_data = json.loads(json_str)
                    except json.JSONDecodeError as e:
                        print(f"L·ªói parse JSON cho object {object_id}: {e}")
                        continue
                
                elif line.startswith('summarization:'):
                    summarization = line.replace('summarization:', '').strip()
            
            if summarization:
                # Tr√≠ch xu·∫•t UN number t·ª´ summarization
                un_match = re.search(r'UN\s+(\d+)', summarization)
                un_number = un_match.group(1) if un_match else "Unknown"
                
                # Tr√≠ch xu·∫•t t√™n ch·∫•t t·ª´ summarization
                name_match = re.search(r'UN\s+\d+:\s*([^;]+)', summarization)
                substance_name = name_match.group(1).strip() if name_match else "Unknown"
                
                return {
                    "original_data": original_data,
                    "summarization": summarization,
                    "un_number": un_number,
                    "substance_name": substance_name
                }
            
            return None
            
        except Exception as e:
            print(f"L·ªói khi parse object {object_id}: {e}")
            return None
    
    def _create_simple_chunk(self, parsed_data, file_path):
        """T·∫°o chunk ƒë∆°n gi·∫£n ch·ªâ v·ªõi summarization"""
        try:
            # T·∫°o chunk content ƒë∆°n gi·∫£n
            chunk_lines = [
                "DANGEROUS GOODS LIST - IMDG CODE",
                "=" * 50,
                f"UN Number: {parsed_data['un_number']}",
                f"Substance: {parsed_data['substance_name']}",
                "",
                "Summary:",
                parsed_data['summarization'],
                ""
            ]
            
            return "\n".join(chunk_lines)
            
        except Exception as e:
            print(f"L·ªói khi t·∫°o chunk: {e}")
            return None

class Loader:
    def __init__(self, supported_types: List[str] = ["pdf", "md", "txt"]) -> None:
        print(f"Loader ƒë∆∞·ª£c kh·ªüi t·∫°o v·ªõi c√°c lo·∫°i file h·ªó tr·ª£: {supported_types}")
        self.supported_types = supported_types
        self.loaders = {
            "pdf": PDFLoader(),
            "md": MarkdownLoader(),
            "txt": DGLTextLoader()  # S·ª≠ d·ª•ng DGLTextLoader cho file .txt
        }
        self.doc_splitter = TextSplitter()

    def load_single_file(self, file_path: str) -> List[Document]:
        """Load v√† x·ª≠ l√Ω m·ªôt file ƒë∆°n l·∫ª"""
        file_extension = Path(file_path).suffix.lower().lstrip('.')
        
        if file_extension not in self.supported_types:
            print(f"B·ªè qua file kh√¥ng h·ªó tr·ª£: {file_path}")
            return []
            
        if file_extension not in self.loaders:
            print(f"Kh√¥ng c√≥ loader cho lo·∫°i file: {file_extension}")
            return []
            
        try:
            print(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω file: {os.path.basename(file_path)}")
            start_time = time.time()
            
            # Load documents
            doc_loaded = self.loaders[file_extension](file_path)
            
            if not doc_loaded:
                print(f"Kh√¥ng th·ªÉ load file: {os.path.basename(file_path)}")
                return []
            
            # Split documents
            doc_split = self.doc_splitter(doc_loaded)
            
            end_time = time.time()
            print(f"Ho√†n th√†nh x·ª≠ l√Ω {os.path.basename(file_path)} trong {end_time - start_time:.2f}s - {len(doc_split)} chunks")
            
            return doc_split
            
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω file {os.path.basename(file_path)}: {str(e)}")
            return []

    def load(self, file_path: Union[str, List[str]]) -> List[Document]:
        """Load m·ªôt file ho·∫∑c danh s√°ch file"""
        if isinstance(file_path, str):
            return self.load_single_file(file_path)
        elif isinstance(file_path, list):
            return self.load_multiple_files(file_path)
        else:
            raise ValueError("file_path ph·∫£i l√† string ho·∫∑c list c·ªßa strings")

    def load_multiple_files(self, file_paths: List[str], max_workers: int = 4) -> List[Document]:
        """Load v√† x·ª≠ l√Ω nhi·ªÅu file song song"""
        print(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(file_paths)} file v·ªõi {max_workers} workers")
        
        all_documents = []
        completed_files = 0
        failed_files = 0
        
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit t·∫•t c·∫£ tasks
            future_to_file = {
                executor.submit(self.load_single_file, file_path): file_path 
                for file_path in file_paths
            }
            
            # X·ª≠ l√Ω k·∫øt qu·∫£ khi ho√†n th√†nh
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    documents = future.result()
                    if documents:
                        all_documents.extend(documents)
                        completed_files += 1
                    else:
                        failed_files += 1
                        
                    # Progress update
                    total_processed = completed_files + failed_files
                    progress = (total_processed / len(file_paths)) * 100
                    print(f"Ti·∫øn ƒë·ªô: {total_processed}/{len(file_paths)} ({progress:.1f}%) - "
                          f"Th√†nh c√¥ng: {completed_files}, Th·∫•t b·∫°i: {failed_files}")
                          
                except Exception as e:
                    failed_files += 1
                    print(f"L·ªói khi x·ª≠ l√Ω file {os.path.basename(file_path)}: {str(e)}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"\n--- K·∫æT QU·∫¢ X·ª¨ L√ù H√ÄNG LO·∫†T ---")
        print(f"T·ªïng s·ªë file: {len(file_paths)}")
        print(f"Th√†nh c√¥ng: {completed_files}")
        print(f"Th·∫•t b·∫°i: {failed_files}")
        print(f"T·ªïng chunks: {len(all_documents)}")
        print(f"Th·ªùi gian x·ª≠ l√Ω: {total_time:.2f}s")
        print(f"T·ªëc ƒë·ªô trung b√¨nh: {len(file_paths)/total_time:.2f} file/s")
        
        return all_documents
    
    def load_dir(self, dir_path: str, max_workers: int = 4, recursive: bool = False) -> List[Document]:
        """Load t·∫•t c·∫£ file ƒë∆∞·ª£c h·ªó tr·ª£ trong th∆∞ m·ª•c"""
        print(f"ƒêang t√¨m ki·∫øm file trong th∆∞ m·ª•c: {dir_path}")
        
        all_files = []
        
        for file_type in self.supported_types:
            if recursive:
                pattern = f"{dir_path}/**/*.{file_type}"
                files = glob.glob(pattern, recursive=True)
            else:
                pattern = f"{dir_path}/*.{file_type}"
                files = glob.glob(pattern)
            
            all_files.extend(files)
            print(f"T√¨m th·∫•y {len(files)} file .{file_type}")
        
        if not all_files:
            print(f"Kh√¥ng t√¨m th·∫•y file n√†o ƒë∆∞·ª£c h·ªó tr·ª£ trong {dir_path}")
            return []
            
        print(f"T·ªïng c·ªông t√¨m th·∫•y {len(all_files)} file")
        
        return self.load_multiple_files(all_files, max_workers)
    
    def get_file_info(self, dir_path: str, recursive: bool = False) -> dict:
        """L·∫•y th√¥ng tin v·ªÅ c√°c file trong th∆∞ m·ª•c"""
        info = {
            "total_files": 0,
            "by_type": {},
            "file_list": []
        }
        
        for file_type in self.supported_types:
            if recursive:
                pattern = f"{dir_path}/**/*.{file_type}"
                files = glob.glob(pattern, recursive=True)
            else:
                pattern = f"{dir_path}/*.{file_type}"
                files = glob.glob(pattern)
            
            info["by_type"][file_type] = len(files)
            info["file_list"].extend(files)
            info["total_files"] += len(files)
        
        return info

if __name__ == "__main__":
    from vector_db import VectorDB
    
    # Kh·ªüi t·∫°o
    loader = Loader(supported_types=["txt"])
    # vector_db = VectorDB()
    
    data_path = "/mnt/d/Workspace/Dangerous_Good_List/DGL_backend_full/dangerous-goods-service/data/part1_output.txt"
    file_info = loader.get_file_info(data_path)
    print("=== TH√îNG TIN THU M·ª§C ===")
    print(f"T·ªïng s·ªë file: {file_info['total_files']}")
    for file_type, count in file_info['by_type'].items():
        print(f"File .{file_type}: {count}")
    
    # Load t·∫•t c·∫£ file trong th∆∞ m·ª•c v·ªõi 2 workers
    print("\n=== B·∫ÆT ƒê·∫¶U LOAD H√ÄNG LO·∫†T ===")
    chunks = loader.load_single_file(data_path)
    print(f"\nK·∫æT QU·∫¢ CU·ªêI C√ôNG: {len(chunks)} chunks t·ªïng c·ªông")
    
    # L∆∞u k·∫øt qu·∫£
    output_file = "/mnt/d/Workspace/Dangerous_Good_List/DGL_backend_full/dangerous-goods-service/data/chunks_part1.txt"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"=== T·ªîNG K·∫æT ===\n")
        f.write(f"S·ªë l∆∞·ª£ng chunks: {len(chunks)}\n")
        f.write(f"C√°c file ƒë∆∞·ª£c x·ª≠ l√Ω: {file_info['file_list']}\n\n")
        
        for i, chunk in enumerate(chunks):
            f.write(f"---- CHUNK {i+1} START ----\n")
            f.write(f"File: {chunk.metadata.get('filename', 'Unknown')}\n")
            f.write(f"Page: {chunk.metadata.get('page', 'Unknown')}\n")
            f.write(chunk.page_content + "\n")
            f.write(f"Metadata: {chunk.metadata}\n")
            f.write(f"---- CHUNK {i+1} END ----\n\n")
    
    print(f"ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o {output_file}")
    
    # Load markdown file
    # file_path = "/mnt/d/Downloads/summaries_dgl.txt"
    # chunks = loader.load_single_file(file_path)
    
    # if chunks:        
    #     # L∆∞u v√†o vector store
    #     success = vector_db.add_data(chunks)
        
    
    #     print(f"‚úÖ ƒê√£ l∆∞u {len(chunks)} chunks v√†o vector store")
        
    #     # Test t√¨m ki·∫øm chi ti·∫øt
    #     query = "what is UN 0004"
    #     print(f"\nüîç T√¨m ki·∫øm chi ti·∫øt: '{query}'")
    #     print("=" * 80)
        
    #     retriever = vector_db.get_compressed_retriever()
    #     results = retriever.invoke(query)
        
    #     for i, chunk in enumerate(results):  
    #         print(f"\n--- CHUNK {i+1} ---")
    #         print(f"UN Number: {chunk.metadata.get('un_number')}")
    #         print(f"Substance: {chunk.metadata.get('substance_name')}")
    #         print(f"Content:\n{chunk.page_content}")
    #         print("="*60)
    # else:
    #     print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file txt")
    
    # query = "I need information about 2309"
    # retriever = vector_db.get_compressed_retriever()
    # results = retriever.invoke(query)
    # vector_db.pretty_print_docs_langchain(results)
        
